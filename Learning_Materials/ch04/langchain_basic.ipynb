{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# <1> 모델 I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### 1. 프롬프트 생성\n",
    "- PromptTemplate\n",
    "- ChatPromptTemplate : 대화형, system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'카메라를 홍보하기 위한 좋은 문구를 추천해줘'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) 프롬프트 생성\n",
    "from langchain import PromptTemplate\n",
    "template = \"{product}를 홍보하기 위한 좋은 문구를 추천해줘\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "\tinput_variables=[\"product\"], \n",
    "\ttemplate=template\n",
    ")\n",
    "\n",
    "prompt.format(product=\"카메라\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 2. LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "##### 2-1) openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강아지입니다.\n"
     ]
    }
   ],
   "source": [
    "## 2-1) openai\n",
    "# OPENAI_API_KEY\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm1 = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "prompt = \"진희는 강아지를 키우고 있습니다. 진희가 키우고 있는 동물은?\"\n",
    "print(llm1.predict(prompt)) # 왜 invoke가 아닌가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "##### 2-2) huggingface - falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "llm2 = HuggingFaceHub(repo_id = model_name)\n",
    "completion = llm2(prompt)\n",
    "print(completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "\n",
    "### 3. 모델 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3) 모델 성능 비교\n",
    "from langchain.model_laboratory import ModelLaboratory\n",
    "model_lab = ModelLaboratory.from_llms([llm1, llm2])\n",
    "model_lab.compare(\"대한민국의 가을은 몇 월부터 몇 월까지야?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### 4. 파서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - **CommaSeparatedListOutputParser** : (파서) 콤마로 구분하여 결과 반환\n",
    "# - **PydanticOuputParser** : (파서) 정의된 필드 타입에 맞게 자동 변환\n",
    "# - **DatetimeOutputParser** : (파서) 날짜/시간 형태로 결과 반환\n",
    "# - **XMLOutputParser** : (파서) XML 형태로 결과 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['두산 베어스', 'LG 트윈스', '키움 히어로즈', 'SSG 랜더스', 'NC 다이노스', '롯데 자이언츠', 'KIA 타이거즈']\n"
     ]
    }
   ],
   "source": [
    "# CommaSeparatedListOutputParser\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0, max_tokens=2048, model_name='gpt-4o')\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser() # 파서 초기화\n",
    "format_instructions = output_parser.get_format_instructions() # 출력 형식 지정\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "\ttemplate='7개의 팀을 보여줘 {subject}.\\n{format_instructions}',\n",
    "\tinput_variables=['subject'],\n",
    "\tpartial_variables={'format_instructions':format_instructions}\n",
    "\t)\n",
    " \n",
    "query = \"한국의 야구팀은?\"\n",
    "output = llm.predict(text=prompt.format(subject=query))\n",
    "\n",
    "parsed_result = output_parser.parse(output)\n",
    "print(parsed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Doe 30\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"다음 사람 정보를 제공해 주세요:\\n{format_instructions}\",\n",
    "    input_variables=[],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "output = llm.predict(text=prompt.format())\n",
    "parsed = parser.parse(output)\n",
    "\n",
    "print(parsed.name, parsed.age)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-02 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# DatetimeOutputParser\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "\n",
    "parser = DatetimeOutputParser()\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"이번 달 첫 번째 월요일의 날짜를 알려줘.\\n{format_instructions}\",\n",
    "    input_variables=[],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "output = llm.predict(text=prompt.format())\n",
    "parsed = parser.parse(output)\n",
    "\n",
    "print(parsed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cats': [{'cat': [{'name': 'Whiskers'}, {'age': '3'}]}, {'cat': [{'name': 'Mittens'}, {'age': '5'}]}, {'cat': [{'name': 'Shadow'}, {'age': '2'}]}]}\n"
     ]
    }
   ],
   "source": [
    "# XMLOutputParser\n",
    "from langchain.output_parsers import XMLOutputParser\n",
    "\n",
    "parser = XMLOutputParser()\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"XML 형식으로 고양이 정보(name, age)를 알려줘.\\n{format_instructions}\",\n",
    "    input_variables=[],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "output = llm.predict(text=prompt.format())\n",
    "parsed = parser.parse(output)\n",
    "\n",
    "print(parsed)  # ➝ {'name': '나비', 'age': '3'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# <2> 데이터 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "- ETL : 데이터를 한 곳에서 다른 한 곳으로 옮기는 과정\n",
    "    - 추출 (Extract) : 여러 출처로 부터 데이터를 가져옴\n",
    "    - 변환 (Transform) : 추출한 데이터를 분석하고 필요한 형태로 변환\n",
    "    - 적재 (Load) : 변환된 데이터를 데이터 베이스나 웨어하우스에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "- 데이터 연결 구성 요소\n",
    "    - 문서 가져오기 : 다양한 출처에서 문서를 가져오는 것. ETL에서 추출에 해당\n",
    "    - 문서 변환 : 입력 데이터를 청크로 분할 하거나 다시 결합하는 작업. ETL에서 변환에 해당\n",
    "    - 문서 임베딩 : 데이터를 벡터로 변환\n",
    "    - 벡터 저장소 : 변환된 벡터를 저장/관리/검색. ETL에서 적재\n",
    "    - 검색기 : 언어 모델과 결합할 관련 문서를 가져오기 위한 것으로 정보 검색을 위한 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "- RAG 실습 필요 라이브러리\n",
    "    - pypdf : pdf 파일 다루는 데 사용\n",
    "    - tiktoken : 임베딩을 하는 데 사용\n",
    "    - faiss-cpu : 벡터 유사도 검색을 위해 사용\n",
    "    - sentence-transofmrers : 문장 또는 단락을 벡터로 변환하기 위해 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### 1. 파일로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 275 0 (offset 0)\n",
      "Ignoring wrong pointing object 1583 0 (offset 0)\n",
      "Ignoring wrong pointing object 1585 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'iOS Version 18.3.2 (Build 22D82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20250330064714Z00'00'\", 'moddate': \"D:20250330064714Z00'00'\", 'source': './data/프롬프트엔지니어링매뉴얼.pdf', 'total_pages': 244, 'page': 0, 'page_label': '1'}, page_content='ঠ\\n\\u0cd0\\x01\\u0c5f\\x01\\n݂\\n\\u09b4\\nߊ\\n\\x0f\\x17\\x11\\x01\\x0f\\x15\\x13\\x11\\x13\\nҊ\\u0a37\\x01\\x01 \\u0a50\\n\\x01\\x01\\u07b9 \\u0a50'),\n",
       " Document(metadata={'producer': 'iOS Version 18.3.2 (Build 22D82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20250330064714Z00'00'\", 'moddate': \"D:20250330064714Z00'00'\", 'source': './data/프롬프트엔지니어링매뉴얼.pdf', 'total_pages': 244, 'page': 1, 'page_label': '2'}, page_content='')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./data/프롬프트엔지니어링매뉴얼.pdf\")\n",
    "document = loader.load()\n",
    "document[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### 2. 임베딩 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "##### 2-1) openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "## 2-1) openai 임베딩\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(document, embeddings)\n",
    "\n",
    "text = \"이 문장은 임베딩 될 문장입니다\"\n",
    "text_embedding = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "##### 2-2) huggingface 임베딩./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-2) huggingface 임베딩\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all_MiniLM-L6-v2\")\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "print(text_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### 3. 검색기 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '업무에 활용할 수 있는 프롬프트 방법은?', 'result': '업무에 활용할 수 있는 프롬프트 방법으로는 카드뉴스, 4컷 만화, 계획보고서, 인포그래픽, 분석보고서, 검토보고서 등을 위한 프롬프트 엔지니어링 매뉴얼이 있습니다. 각 업무 유형에 맞는 프롬프트를 설계하여 효과적으로 활용할 수 있습니다.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0, model_name='gpt-4o')\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "\tllm = llm,\n",
    "\tchain_type=\"stuff\",\n",
    "\tretriever =retriever)\n",
    "\t# chain_type\n",
    "\t# stuff : 모든 문서를 한 번에 붙여서 전달\n",
    "\t# map_reduce : 각각의 문서를 LLM에 넣어 부분적인 응답 생성 > 응답들을 LLM에 넣어 종합 요약 및 최종 답변\n",
    "\t# refine : 초기 답변 생성 후, 문서들을 하나씩 읽으면서 답변을 점진적으로 보완\n",
    "\t# map_rerank : 문서마다 개별적 응답 생성. score를 각각 부여하고 가장 적합한 응답 선택\n",
    "\t\n",
    "query = \"업무에 활용할 수 있는 프롬프트 방법은?\"\n",
    "result = qa({'query':query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "# <3> 체인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "- 여러 구성 요소를 조합해서 하나의 파이프라인을 구성해주는 역할\n",
    "    - LLMChain\n",
    "    - SequentialChain\n",
    "- LCEL : LLM, Prompt, Tool, Parser 등을 블록처럼 연결해서 하나의 파이프라인처럼 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "##### 1. LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 서울입니다.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name='gpt-4o')\n",
    "template = \"{country}의 수도는 어디야?\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['country'])\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run('대한민국')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "##### 2. +SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Over the past decade, social media platforms have transformed the way people communicate, share ideas, and access news. While these platforms have enabled faster dissemination of information, they have also contributed to the spread of misinformation. In recent years, efforts have been made to regulate content, but the sheer volume of posts makes it difficult to monitor everything. As society becomes increasingly reliant on digital communication, the challenge of balancing freedom of expression with the need for accurate information continues to grow.',\n",
       " 'translation': '지난 10년 동안 소셜 미디어 플랫폼은 사람들이 소통하고, 아이디어를 공유하며, 뉴스를 접하는 방식을 변화시켰습니다. 이러한 플랫폼은 정보의 빠른 전파를 가능하게 했지만, 동시에 잘못된 정보의 확산에도 기여했습니다. 최근 몇 년 동안 콘텐츠를 규제하려는 노력이 이루어졌지만, 게시물의 방대한 양 때문에 모든 것을 감시하기는 어렵습니다. 사회가 디지털 소통에 점점 더 의존하게 됨에 따라 표현의 자유와 정확한 정보의 필요성 간의 균형을 맞추는 과제가 계속해서 커지고 있습니다.',\n",
       " 'summary': '소셜 미디어 플랫폼은 소통 방식을 혁신하고 정보 전파를 가속화했지만, 잘못된 정보 확산 문제를 야기했으며, 표현의 자유와 정확한 정보 제공 간의 균형을 맞추는 것이 중요한 과제로 부각되고 있습니다.'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain1,\n",
    "prompt1 = PromptTemplate(\n",
    "\tinput_variables = ['sentence'],\n",
    "\ttemplate = \"다음 문장을 한글로 번역하세요. {sentence}\"\n",
    ")\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"translation\")\n",
    "\n",
    "# chain2,\n",
    "prompt2 = PromptTemplate.from_template(\n",
    "\t\"다음 문장을 한 문장으로 요약하세요. {translation}\"\n",
    ")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"summary\")\n",
    "\n",
    "# allchain\n",
    "from langchain.chains import SequentialChain\n",
    "all_chain = SequentialChain(\n",
    "\tchains=[chain1, chain2],\n",
    "\tinput_variables=['sentence'],\n",
    "\toutput_variables=['translation','summary']\n",
    ")\n",
    "\n",
    "sentence = '''Over the past decade, social media platforms have transformed the way people communicate, share ideas, and access news. While these platforms have enabled faster dissemination of information, they have also contributed to the spread of misinformation. In recent years, efforts have been made to regulate content, but the sheer volume of posts makes it difficult to monitor everything. As society becomes increasingly reliant on digital communication, the challenge of balancing freedom of expression with the need for accurate information continues to grow.'''\n",
    "all_chain(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "##### 3. LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': AIMessage(content='소셜 미디어 플랫폼은 소통 방식을 혁신했지만 잘못된 정보 확산 문제를 초래했으며, 표현의 자유와 정확한 정보 제공 간의 균형을 맞추는 것이 중요한 과제로 부각되고 있습니다.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 314, 'total_tokens': 368, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_92f14e8683', 'finish_reason': 'stop', 'logprobs': None}, id='run-b7124549-d4b8-4df6-aa8e-ad61ee73c209-0')}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableMap, RunnablePassthrough\n",
    "chain1 = prompt1 | llm\n",
    "chain2 = prompt2 | llm\n",
    "\n",
    "# 전체 체인: LCEL 방식 조합\n",
    "full_chain = (\n",
    "    {\"sentence\": RunnablePassthrough()}  # 입력 그대로 넘김\n",
    "    | RunnableMap({\"translation\": chain1})  # 번역 결과 저장\n",
    "    | RunnableMap({\"summary\": lambda x: chain2.invoke({\"translation\": x[\"translation\"]})})  # 요약\n",
    ")\n",
    "\n",
    "result = full_chain.invoke({\"sentence\": sentence})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번역 결과: content='지난 10년 동안 소셜 미디어 플랫폼은 사람들이 소통하고, 아이디어를 공유하며, 뉴스를 접하는 방식을 변화시켰습니다. 이러한 플랫폼은 정보의 빠른 전파를 가능하게 했지만, 동시에 잘못된 정보의 확산에도 기여했습니다. 최근 몇 년 동안 콘텐츠를 규제하려는 노력이 이루어졌지만, 게시물의 방대한 양 때문에 모든 것을 감시하기는 어렵습니다. 사회가 디지털 커뮤니케이션에 점점 더 의존하게 됨에 따라 표현의 자유와 정확한 정보의 필요성 사이의 균형을 맞추는 과제가 계속해서 커지고 있습니다.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 142, 'prompt_tokens': 116, 'total_tokens': 258, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_432e014d75', 'finish_reason': 'stop', 'logprobs': None} id='run-ce17bd76-ddb3-4463-9459-2933263fead8-0'\n",
      "요약 결과: content='소셜 미디어 플랫폼은 소통 방식을 혁신했지만 잘못된 정보 확산 문제를 초래했으며, 디지털 커뮤니케이션 의존도가 높아짐에 따라 표현의 자유와 정확한 정보 제공 간의 균형을 맞추는 것이 중요해졌습니다.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 313, 'total_tokens': 378, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_92f14e8683', 'finish_reason': 'stop', 'logprobs': None} id='run-32b0549e-d365-487b-a14f-359918447e92-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "full_chain = (\n",
    "    {\"sentence\": RunnablePassthrough()}\n",
    "    | RunnableMap({\n",
    "        \"translation\": chain1,\n",
    "        \"sentence\": lambda x: x[\"sentence\"]  # 원문 유지\n",
    "    })\n",
    "    | RunnableMap({\n",
    "        \"translation\": lambda x: x[\"translation\"],  # 번역 유지\n",
    "        \"summary\": lambda x: chain2.invoke({\"translation\": x[\"translation\"]})\n",
    "    })\n",
    ")\n",
    "\n",
    "result = full_chain.invoke({\"sentence\": sentence})\n",
    "\n",
    "print(\"번역 결과:\", result[\"translation\"])\n",
    "print(\"요약 결과:\", result[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "# <4> 메모리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "- 대화 과정에서 발생하는 데이터를 저장\n",
    "    - 모든 대화 유지\n",
    "    - 최근 K개의 대화 유지\n",
    "    - 대화를 요약해서 유지\n",
    "- ConversationChain 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 진희는 강아지를 한마리 키우고 있습니다.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 진희는 강아지를 한마리 키우고 있습니다.\n",
      "AI: 진희가 강아지를 키우고 있다니 정말 귀엽겠네요! 강아지의 이름은 무엇인가요? 그리고 어떤 종인지도 궁금해요. 강아지들은 각자 독특한 성격과 특성을 가지고 있어서 함께 지내는 시간이 정말 즐거울 것 같아요. 강아지와 함께하는 일상에 대해 더 이야기해 주시면 좋겠어요!\n",
      "Human: 영수는 고양이를 두마리 키우고 있습니다.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 진희는 강아지를 한마리 키우고 있습니다.\n",
      "AI: 진희가 강아지를 키우고 있다니 정말 귀엽겠네요! 강아지의 이름은 무엇인가요? 그리고 어떤 종인지도 궁금해요. 강아지들은 각자 독특한 성격과 특성을 가지고 있어서 함께 지내는 시간이 정말 즐거울 것 같아요. 강아지와 함께하는 일상에 대해 더 이야기해 주시면 좋겠어요!\n",
      "Human: 영수는 고양이를 두마리 키우고 있습니다.\n",
      "AI: 영수가 고양이를 두 마리나 키우고 있다니 정말 멋지네요! 고양이들의 이름은 무엇인가요? 그리고 어떤 품종인지도 궁금해요. 고양이들은 각자 독특한 성격을 가지고 있어서 함께 지내는 시간이 정말 흥미로울 것 같아요. 고양이들이 서로 어떻게 지내는지도 궁금하네요. 고양이와 함께하는 일상이나 재미있는 에피소드가 있다면 들려주세요!\n",
      "Human: 진희와 영수가 키우는 동물은 총 몇마리?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'진희는 강아지 한 마리를, 영수는 고양이 두 마리를 키우고 있으니, 총 세 마리의 동물을 키우고 있네요. 진희와 영수가 함께 동물들과 시간을 보내는 모습이 상상되네요. 동물들과의 일상은 항상 즐겁고 특별한 순간들로 가득할 것 같아요!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "\t\t\tmodel_name = 'gpt-4o'\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "from langchain import ConversationChain\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "\n",
    "conversation.predict(input=\"진희는 강아지를 한마리 키우고 있습니다.\")\n",
    "conversation.predict(input=\"영수는 고양이를 두마리 키우고 있습니다.\")\n",
    "conversation.predict(input=\"진희와 영수가 키우는 동물은 총 몇마리?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 안녕\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 만나서 반가워요. 저는 AI입니다. 어떻게 도와드릴까요?'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "#ConversationBufferMemory\t전체 대화 이력 기억\t완전한 대화 문맥 유지\n",
    "#ConversationSummaryMemory\t이전 대화를 요약하여 기억\t메모리 절약, 장기 대화에 적합\n",
    "#ConversationBufferWindowMemory\t최근 n개만 기억\t최신 대화에만 집중\n",
    "#VectorStoreRetrieverMemory\t벡터 DB 기반 유사도 기억\t복잡한 문맥 대응 가능 (RAG 등\n",
    "\n",
    "memory = ConversationBufferMemory(llm=llm)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "conversation.predict(input = '안녕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 안녕\n",
      "AI: 안녕하세요! 만나서 반가워요. 저는 AI입니다. 어떻게 도와드릴까요?\n",
      "Human: 우리집 고양이 이름은 아쿠랑 마린이야\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'아쿠와 마린이라니, 정말 귀여운 이름이네요! 고양이들은 어떤 성격을 가지고 있나요? 아쿠는 장난꾸러기인가요, 아니면 마린이 더 활발한가요? 고양이들과 함께하는 일상 이야기를 듣고 싶어요!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input = '우리집 고양이 이름은 아쿠랑 마린이야')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 안녕\n",
      "AI: 안녕하세요! 만나서 반가워요. 저는 AI입니다. 어떻게 도와드릴까요?\n",
      "Human: 우리집 고양이 이름은 아쿠랑 마린이야\n",
      "AI: 아쿠와 마린이라니, 정말 귀여운 이름이네요! 고양이들은 어떤 성격을 가지고 있나요? 아쿠는 장난꾸러기인가요, 아니면 마린이 더 활발한가요? 고양이들과 함께하는 일상 이야기를 듣고 싶어요!\n",
      "Human: 우리집 고양이 이름이 뭐라고?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'당신의 고양이 이름은 아쿠와 마린이라고 하셨죠! 정말 사랑스러운 이름이에요. 아쿠와 마린이 어떤 특별한 행동이나 습관이 있는지 궁금하네요. 고양이들과 함께하는 일상 이야기를 더 들려주시면 좋겠어요!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input = '우리집 고양이 이름이 뭐라고?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "# <5> 에이전트/툴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haesa\\AppData\\Local\\Temp\\ipykernel_30048\\242275622.py:14: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(tools, # 에이전트가 접근할 수 있는 툴\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m에드 시런이 태어난 해를 알아야 합니다. 이를 위해 Wikipedia를 사용하겠습니다.  \n",
      "Action: wikipedia  \n",
      "Action Input: 에드 시런  \n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Rolling in the Deep\n",
      "Summary: \"Rolling in the Deep\" is a song by English singer-songwriter Adele from her second studio album, 21 (2011). It is the lead single and opening track on the album. The song was written by Adele and Paul Epworth. The singer herself describes it as a \"dark blues-y gospel disco tune\". In 2011, it was reportedly the biggest crossover hit in the United States since 1985; \"Rolling in the Deep\" gained radio airplay from many different radio formats. It was released on 29 November 2010 as the lead single from 21 in digital download format. The lyrics describe the emotions of a scorned lover. The song's official music video uploaded to YouTube on 30 November 2010 has accumulated over 2.58 billion views and 13 million \"Likes\" as of 19 October 2024, similar to that of Adele's another hit \"Someone like You\" released 10 months later, which Adele considered as having changed her life.\n",
      "\"Rolling in the Deep\" received widespread critical acclaim with praise drawn towards the song's production, its lyrics, and Adele's vocal performance. It represented a commercial breakthrough for Adele, propelling her to global superstardom. The song topped the charts in twelve countries and reached the top 10 in over twenty territories. It was Adele's first number-one song in the United States, reaching the top spot on many Billboard charts, including the Billboard Hot 100 where it was number one for seven weeks. By February 2012, \"Rolling in the Deep\" had sold over 8.7 million copies in the United States, making it the best-selling digital song by a female artist in the US, the second-best-selling digital song in the US and Adele's best-selling single outside her native country, topping her previous best-selling \"Chasing Pavements\". Worldwide, it was the fifth-best-selling digital single of 2011 with sales of 8.2 million copies. As of 2019, with sales of over 20.6 million copies worldwide, \"Rolling in the Deep\" is one of the best-selling digital singles of all-time. The song spent 65 weeks on the chart, making the song at that time the fourth-most weeks spent on the chart, tying the place with Jewel with her double single \"Foolish Games\"/\"You Were Meant for Me\".\n",
      "Its music video received a leading seven nominations at the 2011 MTV Video Music Awards, including for Video of the Year, and won three other awards: Best Editing, Best Cinematography and Best Art Direction. \"Rolling in the Deep\" was also the Billboard Year End Hot 100 Number One Single of 2011. At the 54th Annual Grammy Awards, it won awards for Record of the Year, Song of the Year, and Best Short Form Music Video. Various critics and music publications ranked it as the best song of the year on their end-of-year lists with Rolling Stone ranking it at No. 8 spot on its list of \"The 100 Greatest Songs of the 21st Century\". In 2021, the song was ranked at number 82 on the list of Rolling Stone's 500 Greatest Songs of All Time.\n",
      "\n",
      "Page: 2021 Mnet Asian Music Awards\n",
      "Summary: The 2021 Mnet Asian Music Awards ceremony, also known as 2021 MAMA, organized by CJ E&M and broadcast through its music channel Mnet, took place live on December 11, 2021. The ceremony was held at CJ ENM Contents World in Paju, South Korea. The theme for the ceremony was titled as \"Make Some Noise\". The ceremony was the 23rd in the show's history, and the last to be named as the \"Mnet Asian Music Awards,\" as the show rebranded for the following edition.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mIt seems I did not retrieve the correct information about Ed Sheeran. I will try searching again on Wikipedia for Ed Sheeran's birth year. \n",
      "\n",
      "Action: wikipedia  \n",
      "Action Input: Ed Sheeran  \n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Ed Sheeran\n",
      "Summary: Edward Christopher Sheeran ( SHEER-ən; born 17 February 1991) is an English singer-songwriter. Born in Halifax, West Yorkshire, and raised in Framlingham, Suffolk, he began writing songs around the age of eleven. In early 2011, Sheeran independently released the extended play No. 5 Collaborations Project. He signed with Asylum Records the same year.\n",
      "Sheeran's debut album, + (\"Plus\"), was released in September 2011 and topped the UK Albums Chart. It contained his first hit single, \"The A Team\". In 2012, Sheeran won the Brit Awards for Best British Male Solo Artist and British Breakthrough Act. Sheeran's second studio album, × (\"Multiply\"), topped charts around the world upon its release in June 2014. It was named the second-best-selling album worldwide of 2015. In the same year, × won Album of the Year at the 2015 Brit Awards, and he received the Ivor Novello Award for Songwriter of the Year from the British Academy of Songwriters, Composers and Authors. A single from ×, \"Thinking Out Loud\", earned him the 2016 Grammy Awards for Song of the Year and Best Pop Solo Performance.\n",
      "Sheeran's third album, ÷ (\"Divide\"), was released in March 2017, and was the best-selling album worldwide of 2017. The first two singles from the album, \"Shape of You\" and \"Castle on the Hill\", broke records in a number of countries by debuting in the top two positions of the charts. He also became the first artist to have two songs debut in the US top 10 in the same week. By March 2017, Sheeran had accumulated ten top-10 singles from ÷ on the UK Singles Chart, breaking the record for most top-10 UK singles from one album. His fourth single from ÷, \"Perfect\", reached number one in the US, Australia, and the UK, where it became the Christmas number one in 2017. The world's best-selling artist of 2017, he was named the Global Recording Artist of the Year. Released in 2019, his fourth overall and first collaborative album, No.6 Collaborations Project, debuted at number one in most major markets, and spawned three UK number one singles, \"I Don't Care\", \"Beautiful People\", and \"Take Me Back to London\". His fifth studio album, = (\"Equals\"), topped the charts in most major markets in 2021. His sixth album, - (\"Subtract\"), was released on 5 May 2023, while his seventh album, Autumn Variations, was released on 29 September 2023 under his own record label, Gingerbread Man Records.\n",
      "Sheeran has sold 200 million records worldwide, making him one of the world's best-selling music artists. He has 119 million RIAA-certified units in the US, and two of his albums are in the list of the best-selling albums in UK chart history. In December 2019, the Official Charts Company named him artist of the decade, with the most combined success in the UK album and singles charts in the 2010s. As of July 2024, he is the 3rd most followed artist on Spotify. Beginning in March 2017, his ÷ Tour became the highest-grossing of all time in August 2019. An alumnus of the National Youth Theatre in London, Sheeran's acting roles include appearing in the 2019 film Yesterday, playing himself.\n",
      "\n",
      "Page: Ed Sheeran discography\n",
      "Summary: The discography of English singer-songwriter Ed Sheeran consists of seven studio albums, two compilation albums, seventeen extended plays, one video album, sixty-five singles (including twenty-eight as a featured artist), eight promotional singles, one box set, and seventy-one music videos. Sheeran has sold 200 million records worldwide, making him one of the best-selling music artists in history. According to RIAA, Sheeran is the 15th best-selling digital singles artist in the United States with certified sales of 103 million.\n",
      "Originally an indie artist selling music independently on his own label starting in 2005, Sheeran released nine EPs, steadily gaining public and critical acclaim, resulting in his signing to Atlantic Records in January 2011. Five months later, Sheeran released his first single, \"The A Team\", on 12 June 2011. It debuted a\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mEd Sheeran was born on February 17, 1991. To find out how old he is in 2024, I will calculate the difference between 2024 and his birth year, 1991.\n",
      "\n",
      "Action: Calculator\n",
      "Action Input: 2024 - 1991\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 33\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer. \n",
      "\n",
      "Final Answer: 에드 시런은 1991년에 태어났으며, 2024년에는 33살입니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'에드 시런은 1991년에 태어났으며, 2024년에는 33살입니다.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 에드시런 생년월일을 위키피디아에서 조회 > 계산기를 이용해 나이 계산\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0, model_name='gpt-4o')\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "# Tool Type\n",
    "# python_repl : 파이썬 코드 실행 환경\n",
    "# searx-serach : 오픈 소스 검색 엔진 serx 사용\n",
    "# arxiv : arXiv 논문 검색 등\n",
    "agent = initialize_agent(tools, # 에이전트가 접근할 수 있는 툴\n",
    "\t\tllm,\n",
    "\t\tagent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, # 툴의 용도와 사용 시기를 결정하는 에이전트. 툴마다 설명 제공\n",
    "\t\tdescription='계산이 필요할 때', \n",
    "\t\tverbose=True)\n",
    "agent.run(\"에드 시런이 태어난 해는? 2024년도 현재 에드  시런은 몇 살?\")\n",
    "\n",
    "# AgentType\n",
    "# REACT_DOCSTORE : 질문에 답하기 위해 관련 정보 조회\n",
    "# CONVERSATIONAL_REACT_DESCRIPTION : 메모리를 사용하여 과거에 시도했던 대화 기억\n",
    "# SELF_ASK_WITH_SEARCH : 추가 질문을 스스로 만든 뒤, 검색 툴로 답을 찾아 종합\n",
    "# OPENAI_MULTI_FUNCTIONS : funcion-calling 확장형. 복수 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo find out what the paper with the identifier 1605.08386 is about, I should search for it on arXiv. \n",
      "\n",
      "Action: arxiv\n",
      "Action Input: 1605.08386\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mPublished: 2016-05-26\n",
      "Title: Heat-bath random walks with Markov bases\n",
      "Authors: Caprice Stanley, Tobias Windisch\n",
      "Summary: Graphs on lattice points are studied whose edges come from a finite set of\n",
      "allowed moves of arbitrary length. We show that the diameter of these graphs on\n",
      "fibers of a fixed integer matrix can be bounded from above by a constant. We\n",
      "then study the mixing behaviour of heat-bath random walks on these graphs. We\n",
      "also state explicit conditions on the set of moves so that the heat-bath random\n",
      "walk, a generalization of the Glauber dynamics, is an expander in fixed\n",
      "dimension.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe paper titled \"Heat-bath random walks with Markov bases\" by Caprice Stanley and Tobias Windisch studies graphs on lattice points where edges are defined by a finite set of allowed moves of arbitrary length. The authors demonstrate that the diameter of these graphs on fibers of a fixed integer matrix can be bounded by a constant. They also explore the mixing behavior of heat-bath random walks on these graphs and provide explicit conditions on the set of moves to ensure that the heat-bath random walk, a generalization of the Glauber dynamics, is an expander in fixed dimension.\n",
      "\n",
      "Final Answer: The paper \"Heat-bath random walks with Markov bases\" studies graphs on lattice points with edges from a finite set of moves, showing bounded diameter and exploring heat-bath random walks' mixing behavior, providing conditions for the walk to be an expander.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The paper \"Heat-bath random walks with Markov bases\" studies graphs on lattice points with edges from a finite set of moves, showing bounded diameter and exploring heat-bath random walks\\' mixing behavior, providing conditions for the walk to be an expander.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = load_tools([\"wikipedia\", \"llm-math\", \"arxiv\"], llm=llm)\n",
    "# Tool Type\n",
    "# python_repl : 파이썬 코드 실행 환경\n",
    "# searx-serach : 오픈 소스 검색 엔진 serx 사용\n",
    "# arxiv : arXiv 논문 검색 등\n",
    "agent = initialize_agent(tools, # 에이전트가 접근할 수 있는 툴\n",
    "\t\tllm,\n",
    "\t\tagent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, # 툴의 용도와 사용 시기를 결정하는 에이전트. 툴마다 설명 제공\n",
    "\t\tdescription='아무때나', \n",
    "\t\tverbose=True)\n",
    "agent.run(\"What's the paper 1605.08386 about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


# import streamlit as st
# from langchain.chat_models import ChatOpenAI
# from langchain.prompts import PromptTemplate
# from langchain.document_loaders import PyPDFLoader
# from langchain.schema import HumanMessage
# from langchain.text_splitter import RecursiveCharacterTextSplitter

# from dotenv import load_dotenv
# import os

# # --- í™˜ê²½ ì„¤ì • ---
# load_dotenv()
# openai_api_key = os.getenv("OPENAI_API_KEY")

# # --- LangChain LLM ì„¤ì • ---
# llm = ChatOpenAI(model="gpt-4o", temperature=0.2, openai_api_key=openai_api_key)

# # --- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • ---
# summary_prompt = PromptTemplate.from_template("""
# ë‹¤ìŒ ë¬¸ì„œë¥¼ ì½ê³ , ì£¼ìš” ì£¼ì œë³„ë¡œ í•µì‹¬ ë‚´ìš©ì„ ì •ë¦¬í•´ì¤˜.
# ê° ì£¼ì œëŠ” ì œëª©(í—¤ë“œë¼ì¸) í˜•ì‹ìœ¼ë¡œ êµ¬ë¶„í•˜ê³ , ê·¸ ì•„ë˜ì— ê°„ë‹¨í•œ ì„¤ëª…ì„ ë§ë¶™ì—¬ì¤˜.
# ë§ˆì¹˜ ë‰´ìŠ¤ ê¸°ì‚¬ì²˜ëŸ¼ ë¬¸ì„œì˜ ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ í•´ì¤˜.

# ì˜ˆì‹œ í˜•ì‹:
# # [ì£¼ì œ1] (1~10p)
# - í•µì‹¬ ë‚´ìš© ìš”ì•½

# # [ì£¼ì œ2] (11~25p)
# - í•µì‹¬ ë‚´ìš© ìš”ì•½

# ë¬¸ì„œ:
# {text}
# """)

# simplify_prompt = PromptTemplate.from_template("""
# ë‹¤ìŒ ë‚´ìš©ì„ ëˆ„êµ¬ë‚˜ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰¬ìš´ í‘œí˜„ìœ¼ë¡œ ë°”ê¿”ì¤˜. íŠ¹íˆ ìƒˆë¡œìš´ ì£¼ì œì— ëŒ€í•´ í¥ë¯¸ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ ì •ë¦¬í•´ì£¼ë©´ ì¢‹ê² ì–´.

# ê·¸ë¦¬ê³  ìƒˆë¡­ê±°ë‚˜ ì–´ë ¤ìš´ ë‹¨ì–´ëŠ” ì¶”ê°€ë¡œ ì„¤ëª…í•´ì¤˜.

# ë‚´ìš©:
# {summary}
# """)

# # --- í…ìŠ¤íŠ¸ ë¶„í•  ---
# def split_text_into_chunks(text, chunk_size=2000, overlap=100):
#     splitter = RecursiveCharacterTextSplitter(
#         chunk_size=chunk_size,
#         chunk_overlap=overlap,
#         separators=["\n\n", "\n", ".", " "]
#     )
#     return splitter.split_text(text)

# # --- PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
# def extract_text(file_path):
#     loader = PyPDFLoader(file_path)
#     pages = loader.load_and_split()
#     return "\n".join([page.page_content for page in pages])

# # --- Streamlit UI ---
# st.title("ğŸ“„ ë¬¸ì„œ ìš”ì•½ ë° ì‰¬ìš´ ë§ ë³€í™˜ê¸°")

# uploaded_file = st.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])

# if uploaded_file:
#     with st.spinner("PDF ì²˜ë¦¬ ì¤‘..."):
#         with open("temp.pdf", "wb") as f:
#             f.write(uploaded_file.getvalue())

#         text = extract_text("temp.pdf")
#         chunks = split_text_into_chunks(text)

#         summaries = []
#         for i, chunk in enumerate(chunks):
#             try:
#                 prompt_filled = summary_prompt.format(text=chunk)
#                 partial_summary = llm([HumanMessage(content=prompt_filled)]).content
#                 summaries.append(partial_summary)
#             except Exception as e:
#                 summaries.append(f"[ìš”ì•½ ì‹¤íŒ¨ {i+1}] {e}")

#         final_summary = "\n".join(summaries)

#         simplify_prompt_filled = simplify_prompt.format(summary=final_summary)
#         simplified = llm([HumanMessage(content=simplify_prompt_filled)]).content

#         st.subheader("ğŸ“Œ ë¬¸ì„œ ìš”ì•½")
#         st.write(final_summary)

#         st.subheader("ğŸ§’ ì‰¬ìš´ ë§ë¡œ ì„¤ëª…")
#         st.write(simplified)

# easy_pdf_summarizer.py
import os
import requests
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.schema import Document
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ------------------ í™˜ê²½ ì„¤ì • ------------------ #
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GNEWS_API_KEY = os.getenv("GNEWS_API_KEY")

# ------------------ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ------------------ #
def fetch_news(query="í•œêµ­ ê²½ì œ", max_articles=5):
    url = (
        f"https://gnews.io/api/v4/search?q={query}&lang=ko&country=kr&max={max_articles}&apikey={GNEWS_API_KEY}"
    )
    res = requests.get(url)
    articles = res.json().get("articles", [])
    documents = []
    for a in articles:
        content = a.get("content") or a.get("description") or ""
        metadata = {
            "title": a.get("title", ""),
            "url": a.get("url", "")
        }
        documents.append(Document(page_content=content, metadata=metadata))
    return documents

# ------------------ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ------------------ #
def build_news_vectorstore():
    st.info("ğŸ“¡ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
    news_docs = fetch_news()
    if not news_docs:
        st.error("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨. API í‚¤ ë˜ëŠ” ì—°ê²° í™•ì¸.")
        return None
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    news_chunks = splitter.split_documents(news_docs)
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectordb = FAISS.from_documents(news_chunks, embedding=embeddings)
    vectordb.save_local("news_vectordb")
    return vectordb

# ------------------ PDF ë¡œë”© ------------------ #
def load_pdf(uploaded_file):
    save_path = f"tmp/{uploaded_file.name}"
    with open(save_path, "wb") as f:
        f.write(uploaded_file.read())
    loader = PyPDFLoader(save_path)
    return loader.load()

# ------------------ ì‰¬ìš´ ë§ ìš”ì•½ ìƒì„± ------------------ #
def summarize_easy(pdf_docs, retriever):
    pdf_text = " ".join([doc.page_content for doc in pdf_docs])
    system_prompt = """
    ë„ˆëŠ” ì´ˆë“±í•™êµ ì„ ìƒë‹˜ì´ì•¼. ì•„ë˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë„ í•¨ê»˜ ì°¸ê³ í•´ì„œ,
    ì´ PDF ë‚´ìš©ì„ ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì•„ì£¼ ì‰½ê²Œ, í•µì‹¬ë§Œ ìš”ì•½í•´ì¤˜.
    
    ìš”ì•½ì€ ì•„ë˜ì™€ ê°™ì€ êµ¬ì¡°ë¥¼ ë”°ë¼ì¤˜:
    
    1. ë¬¸ì„œ ì „ì²´ì˜ ê°œìš”ë¥¼ ë¨¼ì € ì„¤ëª…í•´ì¤˜ (ë¬´ì—‡ì— ê´€í•œ ë¬¸ì„œì¸ì§€, ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€)
    2. ì£¼ìš” ë‚´ìš©ì„ ì‰½ê²Œ ì •ë¦¬í•´ì¤˜
    3. ìƒˆë¡­ê²Œ ì•Œê²Œ ëœ ì •ë³´ ì¤‘ì—ì„œ í¥ë¯¸ë¡œìš´ ì ì„ ì•Œë ¤ì¤˜

    í˜•ì‹ì€ ì•„ë˜ì²˜ëŸ¼ ì¨ì¤˜:
    ğŸ“˜ ë¬¸ì„œ ê°œìš”:
    ...

    ğŸ§  í•µì‹¬ ë‚´ìš©:
    ...

    ğŸ’¡ í¥ë¯¸ë¡œìš´ ì :
    """

    llm = ChatOpenAI(
        temperature=0.2, model="gpt-4o", openai_api_key=OPENAI_API_KEY
    )
    
    prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template=system_prompt + "\n\nPDF ë‚´ìš©: {context}\n\nì§ˆë¬¸: {question}"
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={
            "prompt": (
                prompt_template
            ),
        },
        return_source_documents=True,
    )
    
    output = qa_chain.invoke({"query": "ì´ ë¬¸ì„œë¥¼ ê´€ì‹¬ì—†ë˜ ì‚¬ëŒë„ í¥ë¯¸ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆê²Œ ìš”ì•½í•´ì¤˜."})
    
    return output

# ------------------ Streamlit ì•± ------------------ #
st.set_page_config(page_title="PDF ì‰¬ìš´ ìš”ì•½ê¸°", layout="centered")
st.title("ğŸ“ ë‰´ìŠ¤ ì°¸ì¡° PDF ì‰¬ìš´ ìš”ì•½ê¸°")

uploaded_file = st.file_uploader("ğŸ“„ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

if uploaded_file:
    st.success("âœ… PDF ì—…ë¡œë“œ ì™„ë£Œ")

    if not os.path.exists("news_vectordb"):
        st.warning("ğŸ§  ë‰´ìŠ¤ ë²¡í„° ì €ì¥ì†Œê°€ ì—†ì–´, ì§€ê¸ˆ ìƒì„±í• ê²Œìš”...")
        build_news_vectorstore()

    # PDF ì²˜ë¦¬
    pdf_docs = load_pdf(uploaded_file)

    # ë²¡í„° ë¶ˆëŸ¬ì˜¤ê¸°
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectordb = FAISS.load_local(
        "news_vectordb", embeddings, allow_dangerous_deserialization=True
    )
    retriever = vectordb.as_retriever(search_kwargs={"k": 3})

    if st.button("ğŸª„ ì‰¬ìš´ ìš”ì•½ ìƒì„±í•˜ê¸°"):
        with st.spinner("ìš”ì•½ ì¤‘..."):
            result = summarize_easy(pdf_docs, retriever)
            summary = result["result"]
            sources = result["source_documents"]
        st.subheader("ğŸ“˜ ì‰¬ìš´ ìš”ì•½ ê²°ê³¼")
        st.write(summary)
        
        # ì°¸ì¡° ë‰´ìŠ¤ ê¸°ì‚¬
        st.subheader("ğŸ“° ìš”ì•½ì— ì°¸ì¡°ëœ ë‰´ìŠ¤ ê¸°ì‚¬")
        for i, doc in enumerate(sources, 1):
            title = doc.metadata.get("title", f"ê¸°ì‚¬ {i}")
            url = doc.metadata.get("url", None)

            if url:
                st.markdown(f"[{title}]({url})")
            else:
                st.markdown(f"ğŸ”— {title} (ë§í¬ ì—†ìŒ)")
else:
    st.info("ì™¼ìª½ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")